{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anemia Detection and Severity Classification in Kenya : A predictive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Kenya Medical Research Institute(KEMRI) is a state corporation established in Kenya in 1979 through the Science and Technology Act, Cap 250 of the Laws of Kenya. It operates under the Science Technology and Innovation Act, 2013 as the national body responsible for carrying out research in human health in Kenya. KEMRI is the medical research arm of the government and provides advice to the Ministry of Health(MOH) on various aspects of healthcare and delivery.\n",
    "\n",
    "KEMRI's mission is to improve human health and quality of life through research , capacity builging , innovation and service delivery. The Institute conducts research under seven research programs including Biotechnology, Public Health and Health Systems, Sexual Reproductive Adolescent and Child health,Natural Products and Drug Development, Infectios and Parasitic Diseases and Non-Communicable Diseases.\n",
    "\n",
    "Within the Health Sector, KEMRI is responsible for providing leadership in health research and development, shaping the health research agenga, setting norms and standards, articulating evidence-based policy options, monitoring and assessing health trends as well as dealing with trans-boundary threats and disease outbreaks. The institute further continues to be responsive the challenges of emerging and re-emerging diseases including Non-Communicable Diseases and communicable conditions and bio-terrorism.\n",
    "\n",
    "Anemia is one of the diseaes that have posed a significant public health problem. Anemia is a condition in which the body lacks enough red blood cells to carry oxygen to the body's tissues. According to the World Health Organization, anemia is a serious global public health problem that particularly affects young children, mensturating adolescent girls, pregnant and postpaturm women.Anemia can be classified in three different levels based on its severity. The severity of the anemia is determined by measuring the level of haemoglobin in the blood.These levels are mild,moderate and severe.\n",
    "\n",
    "In this project we aim to develop predictive models utilizing factors such as Red blood Cell count,age, sex,white bllod cell count etc to help predict the presence of anemia and its severity in different persons. By critically analyzing this different factors we will be able to help KEMRI which was commisioned by the Ministry of Health to improve on the current diagnostic tools on anemia detection in terms of efficiency and accuracy. By doing this we aim to encourage accurate early predictions which may ultimately lead to accelerated interventions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficient and accurate anemia detection, coupled with precise severity measurement, presents a critical challenge in modern healthcare, especially in the context of Kenya's medical landscape. The current diagnostic tools lack the precision needed for early identification, leading to delayed interventions, suboptimal patient outcomes, and escalated healthcare costs due to preventable hospitalizations. In response to this, the Kenya Medical Research Institute (KEMRI), commissioned by the Ministry of Health (MOH), is embarking on a pioneering project. Our goal is to develop predictive models utilizing comprehensive blood count (CBC) parameters, age, and gender data, harnessing the power of machine learning. These models will ease the identification of anemia, to the benefit of patients, healthcare proffesionals, KEMRi and the MOH. Solving this problem may significantly enhance patient care, improve medical decision-making, and alleviate healthcare burdens, contributing data-driven insights to inform healthcare policies and strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Defining Metrics of Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictive model will be successful if it provides a well defined picture of health care utilization in respect to age, gender and the Comprehensive Blood Count in the blood sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>•Based on CBC, age, and gender, how can we forecast the presence of anemia and its severity as early as possible?\n",
    "<br>•Which age group of the population is most susceptible to anemia?\n",
    "<br>•What are the effects the symptoms and impacts of Anemia to a human body?\n",
    "<br>•What effects does anemia have on the various genders?\n",
    "<br>•What effects does anemia have on people's performance and quality of life?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Determine normal reference ranges for key CBC parameters in this population overall and stratified by age and sex\n",
    "2. Examine the relationship between age and abnormal CBC findings\n",
    "3. Calculate the proportion of patients with abnormal results for each CBC parameter\n",
    "4. Compare the prevalence of abnormal CBC results between males and females\n",
    "5. Analyze the association between RBC indices and anemia to understand RBC characteristics in anemic patients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Data Understanding\n",
    "The dataset is derived from complete blood count (CBC) tests performed using a Hematology analyzer. The dataset comprises 11 attributes for each patient, including age, gender, and various CBC parameters such as hemoglobin (HGB) level, mean cell volume (MCV), mean cell hemoglobin (MCH), mean cell hemoglobin concentration (MCHC), red cell distribution width (RDW), red blood cell count (RBC), white blood cell count (WBC), platelet count (PLT), and packed cell volume (PCV). The attributes have specific ranges associated with normal values for each parameter. These ranges provide context for evaluating the health status of patients and identifying deviations from the normal ranges that could indicate the presence of anemia.\n",
    "\n",
    "1. **Age:** Numerical attribute representing the current age of the patients. The age range is between 11 and 100 years.\n",
    "\n",
    "2. **Gender:** Categorical attribute representing the gender of the patient. The possible values are Male and Female.\n",
    "\n",
    "3. **Hemoglobin (HGB):** Numerical attribute indicating the level of hemoglobin in the blood. The normal range for hemoglobin is between 11 and 16 g/dL.\n",
    "\n",
    "4. **Mean Cell Volume (MCV):** Numerical attribute indicating the mean volume of a red blood cell. The normal range for MCV is between 80 and 101 fL.\n",
    "\n",
    "5. **Mean Cell Hemoglobin (MCH):** Numerical attribute indicating the mean amount of hemoglobin in a red blood cell. The normal range for MCH is between 27 and 32 pg.\n",
    "\n",
    "6. **Mean Cell Hemoglobin Concentration (MCHC):** Numerical attribute indicating the mean concentration of hemoglobin in a red blood cell. The normal range for MCHC is between 31 and 37 g/dL.\n",
    "\n",
    "7. **Red Cell Distribution Width (RDW):** Numerical attribute indicating the variation in the size of red blood cells. The normal range for RDW is between 11 and 16%.\n",
    "\n",
    "8. **Red Blood Cell Count (RBC):** Numerical attribute indicating the count of red blood cells per microliter of blood. The normal range for RBC is between 3.80 and 4.80 million/uL.\n",
    "\n",
    "9. **White Blood Cell Count (WBC):** Numerical attribute indicating the count of white blood cells per microliter of blood. The normal range for WBC is between 3.5 and 11.5 thousand/uL.\n",
    "\n",
    "10. **Platelet Count (PLT):** Numerical attribute indicating the count of platelets per microliter of blood. The normal range for platelet count is between 150 and 450 thousand/uL.\n",
    "\n",
    "11. **Packed Cell Volume (PCV):** Numerical attribute indicating the volume percentage of red blood cells in whole blood. The normal range for PCV is between 36 and 46.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score , multilabel_confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import learning_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Reading into the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "df = pd.read_csv(\"CBC data_for_meandeley_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing column names\n",
    "names = [\"S.NO\",\"Age\",\"Sex\",\"Red Blood Cell count\",\n",
    "                 \t\"Packed Cell Volume\",\"Mean Cell Volume\",\n",
    "                    \"Mean Cell Hemoglobin\",\t\"MCHC\",\"Red Cell Distribution width\",\n",
    "                    \"White Blood Cell (WBC count)\",\"Platelet\",\"Hemoglobin\"]\n",
    "df.columns = names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropping the first row and the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the first row\n",
    "df = df.drop([0],axis=0)\n",
    "df.drop(\"S.NO\",axis = 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 11 columns and 373 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datatypes of different columns in the data set\n",
    "datatypes = df.dtypes\n",
    "dname = [ \"DataType\"]\n",
    "dtypedf=pd.DataFrame(datatypes )\n",
    "dtypedf.columns = dname\n",
    "dtypedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert columns to respective data types\n",
    "df[\"Age\"] = df[\"Age\"].astype(float)\n",
    "df[\"Sex\"] = df[\"Sex\"].astype(float)\n",
    "df[\"Red Blood Cell count\"] = df[\"Red Blood Cell count\"].astype(float)\n",
    "df[\"Packed Cell Volume\"] = df[\"Packed Cell Volume\"].astype(float)\n",
    "df[\"Mean Cell Volume\"] = df[\"Mean Cell Volume\"].astype(float)\n",
    "df[\"Mean Cell Hemoglobin\"] = df[\"Mean Cell Hemoglobin\"].astype(float)\n",
    "df[\"MCHC\"] = df[\"MCHC\"].astype(float)\n",
    "df[\"Red Cell Distribution width\"] = df[\"Red Cell Distribution width\"].astype(float)\n",
    "df[\"White Blood Cell (WBC count)\"] = df[\"White Blood Cell (WBC count)\"].astype(float)\n",
    "df[\"Platelet\"] = df[\"Platelet\"].astype(float)\n",
    "df[\"Hemoglobin\"] = df[\"Hemoglobin\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the first five columns of the converted dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the data types of the columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object data types have been converted to floats because objects cannot be directly used in machine learning algorithms hence they are converted into floats to make it easier to use them as inputs in our machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating anemia class column\n",
    "mild_threshold = 10\n",
    "moderate_threshold = 7\n",
    "def classify_anemia(hb_level):\n",
    "    if hb_level >= mild_threshold:\n",
    "        return 'No Anemia'\n",
    "    elif hb_level >= moderate_threshold:\n",
    "        return 'Mild Anemia'\n",
    "    else:\n",
    "        return 'Moderate/Severe Anemia'\n",
    "\n",
    "df['Anemia Classification'] = df['Hemoglobin'].apply(classify_anemia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the first five columns of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new column called Anemia Classification has been created as our target variable to make it easier to predict different classes of anemia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of missing data per column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Put the specified columns in a variable called columns\n",
    "columns = ['Age', 'Sex', 'Red Blood Cell count', 'Packed Cell Volume', 'Mean Cell Volume', 'Mean Cell Hemoglobin', 'MCHC', 'Red Cell Distribution width', 'White Blood Cell (WBC count)', 'Platelet', 'Hemoglobin']\n",
    "\n",
    "# Calculate the median of each column\n",
    "medians = df[columns].median()\n",
    "\n",
    "# Fill missing values in the specified columns with the median of each column\n",
    "df[columns] = df[columns].fillna(medians)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the missing values in the column have been filled with the median to avoid losing valid data as compared to dropping them. They have also been filled with the median because it is less suseptible to outliers as compared to the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = df[df.duplicated()]\n",
    "sample_of_duplicates = duplicate_rows.sample(n=3) \n",
    "\n",
    "print(sample_of_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this indicates that there are 16 duplicates the actual rows are not duplicated they each represent a unique person. The identification as 'duplicates' is attributed to pandas' algorithmic approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Checking for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for outliers using a boxplot\n",
    "selected_columns = [\n",
    "    'Age',\n",
    "    'Sex',\n",
    "    'Red Blood Cell count',\n",
    "    'Packed Cell Volume',\n",
    "    'Mean Cell Volume',\n",
    "    'Mean Cell Hemoglobin',\n",
    "    'MCHC',\n",
    "    'Red Cell Distribution width',\n",
    "    'White Blood Cell (WBC count)',\n",
    "    'Platelet',\n",
    "    'Hemoglobin'\n",
    "]\n",
    "# Determine the number of rows and columns for the grid\n",
    "num_rows = 4\n",
    "num_cols = 3\n",
    "# Create a grid of boxplots\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create individual labeled boxplots for each column\n",
    "for i, column in enumerate(selected_columns):\n",
    "    ax = axes[i]\n",
    "    df.boxplot(column=column, ax=ax)\n",
    "    ax.set_title(f'Boxplot for {column}')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though respective column boxplots indicates outliers in the column data, their retention acknowledges potential distinctions in individual health conditions or measurement precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since haemoglobin is a protein on the surface of the red blood cell. We will understand how they are colerrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking correlation between red blood cell count vs haemoglobin\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data = df,x='Red Blood Cell count', y='Hemoglobin', hue = \"Anemia Classification\")\n",
    "plt.title('Scatter Plot of Hemoglobin vs. Red Blood Cell Count')\n",
    "plt.xlabel('Red Blood Cell Count')\n",
    "plt.ylabel('Hemoglobin')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = np.corrcoef(df['Red Blood Cell count'],df['Hemoglobin'])\n",
    "print(f\"The correlation coefficient is {round(cor[0][1],4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haemoglobin is moderately correlated with red blood cell count. This is because red blood cells contain haemoglobin in them to transport oxygen through the body hence a correlation is expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns_for_scatter = [\n",
    "    'Age',\n",
    "    'Sex',\n",
    "    'Red Blood Cell count',\n",
    "    'Packed Cell Volume',\n",
    "    'Mean Cell Volume',\n",
    "    'Mean Cell Hemoglobin',\n",
    "    'MCHC',\n",
    "    'Red Cell Distribution width',\n",
    "    'White Blood Cell (WBC count)',\n",
    "    'Platelet'\n",
    "]\n",
    "# Create a single figure with subplots\n",
    "num_rows = 5\n",
    "num_cols = 2\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(12, 18))\n",
    "\n",
    "# Flatten the axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create scatter plots for each column\n",
    "for i, column in enumerate(columns_for_scatter):\n",
    "    sns.scatterplot(data=df, x=df[column], y=df['Hemoglobin'], hue=\"Anemia Classification\", ax=axes[i])\n",
    "    axes[i].set_title(f'Scatter Plot of Hemoglobin vs. {column}')\n",
    "    axes[i].set_xlabel(column)\n",
    "    axes[i].set_ylabel('Hemoglobin')\n",
    "    axes[i].grid(True)\n",
    "    axes[i].legend()\n",
    "\n",
    "# Hide any remaining empty subplots\n",
    "for j in range(i + 1, num_rows * num_cols):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Packed cell volume and red blood cell have are highly correlated to the level of haemoglobin in the body. If their level is high then level of haemoglobin will be high.\n",
    "<p> The rest of attribute have uniform to low corelation with haemoglobin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Distibutions of different data in colunms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data = df,x='Hemoglobin',kde = True)\n",
    "plt.title('histogram Plot of Hemoglobin ')\n",
    "plt.xlabel('Red Blood Cell Count')\n",
    "plt.ylabel('count')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemoglobin is normaly distibuted and symmetrical indicating mean, mode and median are close to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_hist = [\n",
    "    'Age',\n",
    "    'Sex',\n",
    "    'Red Blood Cell count',\n",
    "    'Packed Cell Volume',\n",
    "    'Mean Cell Volume',\n",
    "    'Mean Cell Hemoglobin',\n",
    "    'MCHC',\n",
    "    'Red Cell Distribution width',\n",
    "    'White Blood Cell (WBC count)',\n",
    "    'Platelet']\n",
    "# Create a single figure with subplots\n",
    "num_rows = 5\n",
    "num_cols = 2\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(12, 18))\n",
    "\n",
    "# Flatten the axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create histograms for each column\n",
    "for i, column in enumerate(columns_for_hist):\n",
    "    sns.histplot(data=df, x=df[column], ax=axes[i], kde=True)  \n",
    "    axes[i].set_title(f'Histogram of {column}')\n",
    "    axes[i].set_xlabel(column)\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "# Hide any remaining empty subplots\n",
    "for j in range(i + 1, num_rows * num_cols):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most attribute are normaly distributed. <br> Some are right skwed indicating the existence of extrime values <br> MCHC and Age columns are binormal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of columns to include in the bar graph\n",
    "columns_for_bar = [\n",
    "    'Age',\n",
    "    'Red Blood Cell count',\n",
    "    'Packed Cell Volume',\n",
    "    'Mean Cell Volume',\n",
    "    'Mean Cell Hemoglobin',\n",
    "    'MCHC',\n",
    "    'Red Cell Distribution width',\n",
    "    'White Blood Cell (WBC count)',\n",
    "    'Platelet',\n",
    "    'Hemoglobin'\n",
    "]\n",
    "num_rows = 5\n",
    "num_cols = 2\n",
    "# Create a single figure with subplots\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(12, 18))\n",
    "\n",
    "# Flatten the axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create bar graphs for each variable against Anemia Classification\n",
    "for i, column in enumerate(columns_for_bar):\n",
    "    sns.barplot(data=df, x='Anemia Classification', y=column, ax=axes[i])\n",
    "    axes[i].set_title(f'Bar Graph of {column} vs. Anemia Classification')\n",
    "    axes[i].set_xlabel('Anemia Classification')\n",
    "    axes[i].set_ylabel(column)\n",
    "    axes[i].grid(True)\n",
    "    \n",
    "\n",
    "# Hide any remaining empty subplots\n",
    "for j in range(len(columns_for_bar), num_rows * num_cols):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a distribution between sex and anemia class\n",
    "g = sns.catplot(data=df, x='Age', y='Anemia Classification', hue='Sex', kind='bar')\n",
    "g.set_xticklabels(rotation=90)\n",
    "g.set_axis_labels('Age', 'Anemia Classification')\n",
    "g._legend.set_title('Sex')\n",
    "new_labels = ['Female', 'Male']\n",
    "for t, l in zip(g._legend.texts, new_labels): t.set_text(l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above indicates that Mild Anemia is more prevalent in females than it is in males.\n",
    "Moderate/Severe Anemia is more prevalent in males than it is in females\n",
    "There is also a higher number of females who have no anemia compared to males although not by a wide margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Class Balancing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the first five rows of the dataframe\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for the the counts in each class of anemia\n",
    "df[[\"Anemia Classification\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allocating the features and target to X and y\n",
    "X = df[[\"Age\", \"Sex\",\"Red Blood Cell count\",\"Packed Cell Volume\",\"Mean Cell Hemoglobin\",\"MCHC\",\"Red Cell Distribution width\",\"White Blood Cell (WBC count)\",\"Platelet\",\"Hemoglobin\"]]\n",
    "y = df[\"Anemia Classification\"] \n",
    "\n",
    "#printing the original counts of each level\n",
    "print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "#oversampling using smote\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "#printing the new counts of the oversampled values\n",
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)First Model - K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the train and test data\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_res,y_res,test_size = 0.25,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encording the target variables\n",
    "ohe = OneHotEncoder()\n",
    "# encording y_train\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_train_encoded = ohe.fit_transform(y_train).toarray()\n",
    "# encording y_test\n",
    "y_test = np.array(y_test)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "y_test_encoded = ohe.fit_transform(y_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the algorithim\n",
    "classfier = KNeighborsClassifier(n_neighbors = round(np.sqrt(len(df))))\n",
    "classfier.fit(X_train,y_train_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted values\n",
    "y_pred = classfier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for accuracy\n",
    "f1= f1_score(y_test_encoded, y_pred,average = \"micro\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = multilabel_confusion_matrix(y_test_encoded, y_pred)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the best value of our parameter k\n",
    "\n",
    "k_range = range(1, 30)\n",
    "mse = []\n",
    "\n",
    "for k in k_range:\n",
    "    classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    classifier.fit(X_train, y_train_encoded)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Calculate the MSE for this k\n",
    "    mse_value = metrics.mean_squared_error(y_test_encoded, y_pred)\n",
    "    mse.append(mse_value)\n",
    "\n",
    "# Plot the MSE values against different values of k\n",
    "plt.plot(k_range, mse, marker='o')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('KNN Mean Squared Error for Different K Values')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more the value of n_neighbors is increasing the more the mean squared error is increasing so it is advisable to use value less than neighbours for k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we use k as 3 the results seems to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using k as 3 becouse it has the minimum error\n",
    "classfier = KNeighborsClassifier(n_neighbors = 3)\n",
    "classfier.fit(X_train,y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted values\n",
    "y_pred = classfier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1= f1_score(y_test_encoded, y_pred,average = \"micro\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = multilabel_confusion_matrix(y_test_encoded, y_pred)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Second Model - Decison Tree Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "classifier.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the train and test data\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_res,y_res,test_size = 0.25,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test_encoded, y_pred, average=\"micro\")\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = multilabel_confusion_matrix(y_test_encoded, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_encoded.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning using gridsearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the parameter grid you want to search\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Create the Decision Tree classifier\n",
    "classifier = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Create the GridSearchCV instance\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Print the best parameters and the corresponding accuracy\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Gini impurity calculation function\n",
    "def gini_impurity(classes):\n",
    "    unique_classes, class_counts = np.unique(classes, return_counts=True)\n",
    "    class_probs = class_counts / np.sum(class_counts)\n",
    "    gini = 1 - np.sum(class_probs ** 2)\n",
    "    return gini\n",
    "\n",
    "# Generate example data\n",
    "classes = np.array([0, 1, 1, 0, 0, 1, 0, 1])\n",
    "predicted_probabilities = np.linspace(0, 1, 100)\n",
    "\n",
    "# Calculate Gini impurity for different splits\n",
    "gini_values = []\n",
    "for p in predicted_probabilities:\n",
    "    class_predictions = np.where(predicted_probabilities >= p, 1, 0)\n",
    "    gini = gini_impurity(class_predictions)\n",
    "    gini_values.append(gini)\n",
    "\n",
    "# Plot the Gini impurity values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(predicted_probabilities, gini_values, marker='o')\n",
    "plt.xlabel('Predicted Probability Threshold')\n",
    "plt.ylabel('Gini Impurity')\n",
    "plt.title('Gini Impurity for Different Predicted Probability Thresholds')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Model - Gaussian Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the train and test data\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_res,y_res,test_size = 0.4,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Naive Bayes to the Training set \n",
    "Bayes = GaussianNB()  \n",
    "Bayes.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the Test set results  \n",
    "\n",
    "y_pred3 = Bayes.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes_accuracy = accuracy_score(y_test, y_pred3)\n",
    "Bayes_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "cmn = confusion_matrix(y_test, y_pred3)  \n",
    "\n",
    "print(cmn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tune using Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Grid Search Parameters\n",
    "\n",
    "\n",
    "param_grid_nb = {\n",
    "    'var_smoothing': np.logspace(0,-9, num=100)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "NaiveModel_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=1, cv=10)\n",
    "NaiveModel_grid.fit(X_train, y_train)\n",
    "print(NaiveModel_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>estimator is the machine learning model of interest, provided the model has a scoring function; in this case, the model assigned is GaussianNB().\n",
    "\n",
    "<br>param_grid is a dictionary with parameters names (string) as keys and lists of parameter settings to try as values; this enables searching over any sequence of parameter settings.\n",
    "\n",
    "<br>verbose is the verbosity: the higher, the more messages; in this case, it is set to 1.\n",
    "\n",
    "<br>cv is the cross-validation generator or an iterable, in this case, there is a 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on Testing Data\n",
    "y_pred_tune = NaiveModel_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Tune_accuracy_score = accuracy_score(y_test, y_pred_tune)\n",
    "Tune_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "cmn = confusion_matrix(y_test, y_pred_tune)  \n",
    "\n",
    "print(cmn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Fourth Model - Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instatiating the svm classifier with a nin-linear kernel\n",
    "\n",
    "clf = SVC(kernel = \"rbf\")\n",
    "\n",
    "# fitting the classifier to the training data\n",
    "\n",
    "clf.fit(X_train , y_train)\n",
    "\n",
    "# making predictions with the test and train data\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred1 = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the f1 score and the accuracy of the training and testing data\n",
    "#accuracy for the training data \n",
    "acc1 = accuracy_score(y_train , y_pred1)\n",
    "print(f'Trainning accuracy : {acc1 :.2f}')\n",
    "\n",
    "#accuracy for the test data\n",
    "acc2 = accuracy_score(y_test , y_pred)\n",
    "print(f'Testing accuracy : {acc2 :.2f}')\n",
    "\n",
    "#f1 score for the training data\n",
    "f1 = f1_score(y_train, y_pred1, average='weighted')\n",
    "print(f'F1 Score for training data: {f1:.2f}')\n",
    "\n",
    "#f1 score for the testing data\n",
    "f2 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f'F1 Score for testing data: {f2:.2f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score of the model is very low which might indicate underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the model using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline with an SVM classifier\n",
    "pipe = Pipeline([\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "# Defining the hyperparameter grid for the grid search\n",
    "param_grid = {\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__kernel': ['linear', 'rbf'],\n",
    "    'clf__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Create a grid search object\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f'Best hyperparameters: {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting our train and test data\n",
    "\n",
    "y_p_1 = grid.predict(X_train)\n",
    "y_p_2 = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the accuracy anf f1 score of the train and test data\n",
    "\n",
    "#accuracy for the training data \n",
    "acc1 = accuracy_score(y_train , y_p_1)\n",
    "print(f'Trainning accuracy : {acc1 :.2f}')\n",
    "\n",
    "#accuracy for the test data\n",
    "acc2 = accuracy_score(y_test , y_p_2)\n",
    "print(f'Testing accuracy : {acc2 :.2f}')\n",
    "\n",
    "#f1 score for the training data\n",
    "f1 = f1_score(y_train, y_p_1, average='weighted')\n",
    "print(f'F1 Score for training data: {f1:.2f}')\n",
    "\n",
    "#f1 score for the testing data\n",
    "f2 = f1_score(y_test, y_p_2, average='weighted')\n",
    "print(f'F1 Score for testing data: {f2:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameter tuning the model seems to be doing better indicating that is learning from the data.\n",
    "Also the test and the train accuracy have a small margin difference indicating the model is not overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Fourth Model - Gradient Boosted Decision Trees model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.4, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the Gradient Boosted Decision Trees model\n",
    "\n",
    "base_model = GradientBoostingClassifier(random_state=0)\n",
    "base_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_base = base_model.predict(X_test)\n",
    "\n",
    "f1_base = f1_score(y_test, y_pred_base, average='micro')\n",
    "print(f\"f1_score: {f1_base}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "base_accuracy = accuracy_score(y_test, y_pred_base)\n",
    "print(\"Gradient Boosted Decision Trees Accuracy:\", base_accuracy)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix_base = confusion_matrix(y_test, y_pred_base)\n",
    "print(\"Confusion Matrix for Gradient Boosted Decision Trees:\")\n",
    "print(conf_matrix_base)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for Gradient Boosted model\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100, 200],  # You can add more parameters to tune\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Perform grid search for hyperparameter tuning\n",
    "gb_grid = GridSearchCV(estimator=GradientBoostingClassifier(random_state=0),\n",
    "                       param_grid=param_grid_gb, verbose=1, cv=10)\n",
    "gb_grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator from grid search\n",
    "best_gb_model = gb_grid.best_estimator_\n",
    "print(\"Best Gradient Boosted Decision Trees Model:\", best_gb_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the best tuned model\n",
    "y_pred_tuned_gb = best_gb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy for the tuned model\n",
    "tuned_gb_accuracy = accuracy_score(y_test, y_pred_tuned_gb)\n",
    "print(\"Tuned Gradient Boosted Decision Trees Accuracy:\", tuned_gb_accuracy)\n",
    "\n",
    "tuned_gb_f1 = f1_score(y_pred_tuned_gb, y_pred_base, average='micro')\n",
    "print(f\"f1_score: {f1_base}\")\n",
    "\n",
    "# Confusion matrix for the tuned model\n",
    "cmn_tuned_gb = confusion_matrix(y_test, y_pred_tuned_gb)\n",
    "print(\"Confusion Matrix for Tuned Gradient Boosted Decision Trees:\")\n",
    "print(cmn_tuned_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = best_gb_model.feature_importances_\n",
    "\n",
    "# Get feature names from the dataset\n",
    "feature_names = X_train.columns  \n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "# Sort the features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importance Plot')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, valid_scores = learning_curve(best_gb_model, X_train, y_train, cv=10, scoring='accuracy')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training Accuracy')\n",
    "plt.plot(train_sizes, np.mean(valid_scores, axis=1), label='Validation Accuracy')\n",
    "plt.xlabel('Training Data Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter tuning process didn't notably enhance the model's performance on this dataset, suggesting inherent simplicity or near-optimal initial hyperparameters. The learning rate impacts overfitting, complemented by other key hyperparameters like tree count and depth, influencing the model's effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
